<!-- prettier-ignore-start -->

<p align="center">
  <img src="https://github.com/albizujon/asistente-visual/raw/main/static/images/logo.png" alt="Asistente Visual Logo" width="120"/>
  <h1 align="center">Asistente Visual</h1>
  <p align="center">
    <a href="#-descripci√≥n"><img src="https://img.shields.io/badge/versi√≥n-1.0.0-blue.svg" alt="Version Badge"/></a>
    <a href="https://github.com/albizujon/asistente-visual/actions"><img src="https://img.shields.io/github/actions/workflow/status/albizujon/asistente-visual/ci.yml?branch=main&label=build" alt="Build Status"/></a>
    <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/licencia-MIT-green.svg" alt="License Badge"/></a>
    <a href="https://img.shields.io/badge/python-3.8%2B-blue.svg"><img src="https://img.shields.io/badge/python-3.8%2B-blue.svg" alt="Python Version"/></a>
  </p>
</p>

---

## üìù Descripci√≥n

**Asistente Visual** es una aplicaci√≥n full-stack dise√±ada para asistir a personas con discapacidad visual. Combina el poder del **reconocimiento de voz**, **procesamiento de im√°genes** y **modelos de IA** para interpretar escenas y responder preguntas en tiempo real.

> "Un puente entre el mundo visual y auditivo, al alcance de tu voz." 

---

## üìñ Tabla de Contenidos

- [üìù Descripci√≥n](#-descripci√≥n)
- [üìñ Tabla de Contenidos](#-tabla-de-contenidos)
- [‚ú® Caracter√≠sticas](#-caracter√≠sticas)
- [üì∏ Demo](#-demo)
- [üóÇ Estructura del Proyecto](#-estructura-del-proyecto)
- [üõ† Requisitos](#-requisitos)
- [üöÄ Instalaci√≥n](#-instalaci√≥n)
- [‚ñ∂Ô∏è Uso](#Ô∏è-uso)
- [‚òÅÔ∏è Despliegue](#Ô∏è-despliegue)
- [ü§ù Contribuir](#-contribuir)
- [üìú Licencia](#-licencia)

---

## ‚ú® Caracter√≠sticas

- üé• **Captura de imagen en tiempo real** desde la c√°mara del dispositivo.
- üó£ **Reconocimiento de voz en Espa√±ol** (SpeechRecognition API).
- üîç **OCR** con **EasyOCR** para extraer texto.
- üì¶ **Detecci√≥n de objetos** y **colores** con **YOLOv8**.
- ü§ñ **Respuesta inteligible** usando un modelo LLM (Mistral) v√≠a OpenRouter.
- üîä **S√≠ntesis de voz** de la respuesta (SpeechSynthesis API).
- üíæ **Registro de im√°genes** recibidas para auditor√≠a y an√°lisis.

---

## üì∏ Demo

![Demo Asistente Visual](https://github.com/albizujon/asistente-visual/raw/main/static/images/demo.gif)

---

## üóÇ Estructura del Proyecto

| Carpeta/Archivo            | Descripci√≥n                                   |
|----------------------------|-----------------------------------------------|
| `app.py`                   | Servidor Flask principal                      |
| `.env`                     | Variables de entorno (API keys)               |
| `requirements.txt`         | Dependencias Python                           |
| `yolov8n.pt`, `yolov8x.pt` | Modelos YOLOv8                                |
| `imagenes_recibidas/`      | Carpeta para almacenar im√°genes capturadas     |
| `templates/`               | Plantillas HTML                               |
| `static/css/styles.css`    | Estilos CSS                                   |
| `static/js/main.js`        | L√≥gica de c√°mara, voz y comunicaci√≥n HTTP     |
| `static/images/`           | Logos, demo y assets                          |

---

## üõ† Requisitos

- **Python** >= 3.8
- **npm** / **Node.js** (opcional, para bundling frontend)
- Cuenta y **API Key** en [OpenRouter](https://openrouter.ai)

---

## üöÄ Instalaci√≥n

1. **Clona este repositorio**
   ```bash
   git clone https://github.com/albizujon/asistente-visual.git
   cd asistente-visual
   ```

2. **Crea y activa** un entorno virtual
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # macOS / Linux
   source venv/bin/activate
   ```

3. **Instala dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configura** tus credenciales en `.env`
   ```ini
   OPENROUTER_OPENROUTER_API_KEY=tu_api_key
   OPENROUTER_ENDPOINT=https://openrouter.ai/api/v1/chat/completions
   ```

5. **Descarga** los modelos YOLO en la ra√≠z:
   - `yolov8n.pt`
   - `yolov8x.pt`

---

## ‚ñ∂Ô∏è Uso

```bash
python app.py
```

1. Abre en tu navegador o m√≥vil: `http://<IP_DEL_SERVIDOR>:5000/`
2. Concede permisos de c√°mara y micr√≥fono.
3. Haz click en **Hablar y Capturar** y formula tu pregunta.
4. Recibe la respuesta en pantalla y por voz.

---

## ‚òÅÔ∏è Despliegue

- Producci√≥n con **Gunicorn** + **Nginx**
- Certificados SSL con **Let's Encrypt**
- **Docker** & **Docker Compose** para contenerizaci√≥n

---

## ü§ù Contribuir

1. Haz un fork üîÄ
2. Crea tu rama (`git checkout -b feature/nueva-funcion`)
3. Haz commit (`git commit -m 'A√±ade nueva funci√≥n'`)
4. Sube tu rama (`git push origin feature/nueva-funcion`)
5. Abre un Pull Request ‚ù§Ô∏è

---

## üìú Licencia

MIT ¬© 2025 [Jon Albizu](https://github.com/albizujon)

<!-- prettier-ignore-end -->
